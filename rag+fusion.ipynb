{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python3 -m pip install pandas openai sentence-transformers chromadb -> to install pip and frameworks\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"/Users/AdhamMotawi/Downloads/archive/fra_cleaned.csv\",\n",
    "    encoding=\"latin1\",\n",
    "    engine=\"python\",\n",
    "    sep=\";\",\n",
    "    on_bad_lines=\"skip\"  # skips malformed lines\n",
    ")\n",
    "\n",
    "df.fillna(\"\", inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create perfume descriptions\n",
    "def build_description(row):\n",
    "    accords = ', '.join([row[f'mainaccord{i}'] for i in range(1, 6) if row[f'mainaccord{i}']]) # Combine the top 5 main accords into a comma-separated string\n",
    "    \n",
    "    # Create a rich textual description for each perfume based on its features\n",
    "    return (\n",
    "        f\"{row['Perfume']} by {row['Brand']}, released in {int(row['Year']) if row['Year'] else 'unknown'}.\\n\"\n",
    "        f\"Top notes: {row['Top']}.\\n\"\n",
    "        f\"Middle notes: {row['Middle']}.\\n\"\n",
    "        f\"Base notes: {row['Base']}.\\n\"\n",
    "        f\"Main accords: {accords}.\\n\"\n",
    "        f\"Gender: {row['Gender']}. Rating: {row['Rating Value']} ({row['Rating Count']} votes).\"\n",
    "    )\n",
    "\n",
    "# Apply the function to all rows to generate a new 'text' column for embedding\n",
    "df[\"text\"] = df.apply(build_description, axis=1)\n",
    "\n",
    "#Embed the descriptions using a SentenceTransformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\") # Load the pre-trained embedding model (fast + good quality for semantic search)\n",
    "\n",
    "# Generate vector embeddings for each perfume description\n",
    "embeddings = model.encode(df[\"text\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "#Store the embedded data in a ChromaDB vector database (designed for the storing and retrieving vector embeddings) --> store embeddings with associated metadata for subsequent use by language models\n",
    "# Initialise the Chroma client \n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "\n",
    "# Ensure a clean start by deleting any existing 'perfumes' collection\n",
    "chroma_client.delete_collection(name=\"perfumes\")\n",
    "\n",
    "# Create a new collection to store perfume data\n",
    "collection = chroma_client.create_collection(name=\"perfumes\")\n",
    "\n",
    "#Batch insert the data into ChromaDB\n",
    "batch_size = 1000\n",
    "texts = df[\"text\"].tolist() # List of descriptions\n",
    "ids = [str(i) for i in df.index] # Unique string IDs for each perfume\n",
    "metas = df[[\"Perfume\", \"Brand\", \"Gender\"]].to_dict(orient=\"records\") # Metadata for filtering/display\n",
    "\n",
    "\n",
    "# Insert data in batches to avoid API overload\n",
    "for i in range(0, len(df), batch_size):\n",
    "    collection.add(\n",
    "        documents=texts[i:i+batch_size], # The text descriptions\n",
    "        embeddings=embeddings[i:i+batch_size],  #Corresponding vector embeddings\n",
    "        ids=ids[i:i+batch_size], # Unique IDs\n",
    "        metadatas=metas[i:i+batch_size] # Additional metadata (name, brand, gender)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_perfumes(query, k=10): # Function to retrieve the top-k most relevant perfumes for a given user query\n",
    "\n",
    "    #Encode the user's text query into an embedding vector\n",
    "    query_vec = model.encode([query])[0]\n",
    "\n",
    "    #Use ChromaDB to search for the top-k perfumes whose descriptions are most similar to the query\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_vec],\n",
    "        n_results=k\n",
    "    )\n",
    "\n",
    "    #Return the list of retrieved document texts (perfume descriptions)\n",
    "    return results[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "HF_TOKEN = \"your-token-here"",
    "\n",
    "\n",
    "client_llama = InferenceClient(\"meta-llama/Llama-3.1-8B-Instruct\", token=HF_TOKEN)\n",
    "client_mistral = InferenceClient(\"mistralai/Mistral-7B-Instruct-v0.3\", token=HF_TOKEN)\n",
    "client_microsoft = InferenceClient(\"microsoft/Phi-3-mini-4k-instruct\", token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_prompt(query, docs):\n",
    "    context = \"\\n\".join([f\"- {doc}\" for doc in docs]) # Format the list of perfumes into bullet points\n",
    "    return f\"\"\"\n",
    "You are a highly knowledgeable and articulate perfume expert. A user has submitted the following request:\n",
    "\n",
    "\"{query}\"\n",
    "\n",
    "Below is a curated list of perfumes from your database:\n",
    "\n",
    "{context}\n",
    "\n",
    "Your task is to recommend the 2–3 most suitable perfumes **from the list above** that best match the user's request.\n",
    "\n",
    "Please follow these detailed instructions:\n",
    "- Tailor your recommendations precisely to the user’s intent. Consider all aspects: scent profile, gender preference, weather/season, longevity, brand reputation, time of day, or occasion — whatever the query implies.\n",
    "- Avoid redundancy. Do not suggest perfumes that are overly similar or duplicate in style, unless justified.\n",
    "- Clearly explain **why** each perfume matches the user's needs. Max one-two sentence\n",
    "- Present your final picks as a bullet list in the following format:\n",
    "\n",
    "**Recommended Perfumes:**\n",
    "- **[Perfume Name] by [Brand]** – [Explanation of why it fits]\n",
    "- **[Perfume Name] by [Brand]** – [Explanation of why it fits]\n",
    "\n",
    "Be concise but insightful. Do not suggest perfumes outside the list. Avoid vague reasoning. Give the user confidence in your expertise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_judge_prompt(query, r1, r2, r3):\n",
    "    return f\"\"\"\n",
    "The user asked: \"{query}\"\n",
    "\n",
    "Here are responses from three perfume experts:\n",
    "\n",
    "Expert A:\n",
    "{r1}\n",
    "\n",
    "Expert B:\n",
    "{r2}\n",
    "\n",
    "Expert C:\n",
    "{r3}\n",
    "\n",
    "Please combine their responses into one final, concise recommendation. Remove duplicates, justify your top picks, and tailor the answer to the query.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I want a unisex fragrance for summer evenings\"\n",
    "\n",
    "#Retrieve top-k perfume descriptions from ChromaDB based on semantic similarity to the user query.\n",
    "docs = retrieve_perfumes(query, k=10)  # from earlier\n",
    "\n",
    "##This tells the LLM: the options; now pick the best ones and explain why\n",
    "prompt = fusion_prompt(query, docs)\n",
    "\n",
    "resp1 = client_llama.text_generation(prompt, max_new_tokens=350)\n",
    "resp2 = client_mistral.text_generation(prompt, max_new_tokens=350)\n",
    "resp3 = client_microsoft.text_generation(prompt, max_new_tokens=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt = fusion_judge_prompt(query, resp1, resp2, resp3)\n",
    "final_response = client_llama.text_generation(judge_prompt, max_new_tokens=400)\n",
    "\n",
    "print(\"\\n Final Recommendation:\\n\")\n",
    "print(final_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
